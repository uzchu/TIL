# 회귀분석(Regression)

> 인과관계를 파악하기 위한 분석
>
> 평균적인 성향으로 집단의 성격을 설명할 수 있음



#### 1. 회귀식 : 변량이 2개 이상일 때

```
                   Y(종속변수) = w * X(독립변수) + b (+ㅌ(오차항))
```

=> 데이터를 평균적으로 가장 잘 설명할 수 있는 선

​	(즉, 정규분포를 만족할 때 회귀식도 의미가 있음)

=> w와 b값을 최소제곱법(평균제곱오차가 최소가 되는 값을 찾는법)을 이용해 찾음

=> x, y가 정규성을 띌 때, 오차는 대칭적 구조기 때문에 더하면 0이 나옴. 따라서 생략



#### 2. 선형회귀모델

1. 단순선형회귀식

```
y = w*x +b
```

2. 다중회귀식

- x1과 x2는 서로 독립

```
y = w1*x1 + w2*x2 + ... + b
```

3. 다항회귀식

- w1(계수)가 제곱이나 세제곱 등 일 때 비선형모델

```
y = w1*x1^2 + b
```

​	=> y(종속변수)는 무조건 수치형만! 명목형(범주형)안됨.

​	=> x(독립변수)는 명목형(범주형)도 가능하지만 수치형일 때 정확도 높은 분석결과가 잘 나오기 때문에 수치형 자료를 권장함.



#### 3. 회귀분석의 가정 충족

1. 선형성 - 종속변수와 독립변수간 선형관계

   - Superposition (Additivity) : 상수가 영향을 주지 않는다는 의미 `(f(ax)=af(x))`
   - Homogeneity : 균질성 `f(a1x1+a2x2)=a1f(x1)+a2f(x2)`

2. 잔차의 정규성 : 회귀식이 정규분포를 이용한 개념이므로, 잔차의 합은 0이어야 함

   - Q-Q plot : x축에 실측 누적 확률, y축에 예측치 누적확률. 45도의 기울기를 가진 직선에 가까울 수록 오차항이 정규분포를 따른다고 볼 수 있음
   - Shapiro-Wilk 검정방법 : 단일 정규성 검정 > 정규분포를 따르면 w값이 1에 가까워짐

3. 잔차의 등분산성 : 잔차에 특정 패턴이 있으면 안됨 (log변환, 가중최소제곱법으로 이분산성 해결)

   - 잔차의 이분산성 : 독립변수와 오차항이 상관관계가 있다는 의미 (그림d만 정상)

   ![image](https://user-images.githubusercontent.com/58683097/70900355-fa235400-203b-11ea-86aa-4b046e57ee2e.png)

   - 잔차의 이분산성이 회귀식에 미치는 영향 : 표준오차가 과소계상 될 경우, t통계량이 과대평가되어 귀무가설을 기각하는 제1종오류 발생
     - s값(표준오차)가 작아지면 t값이 커짐. (그래프상에서 끝으로) 귀무가설을 기각하게 됨

   **=> 즉, 잔차가 패턴을 가지면 t통계량, f값이 의미가 없어지기 때문에 회귀분석이 의미가 없어짐**

4. 잔차의 독립성 : 오차항간의 상관관계가 있으면 안됨 (상관계수가 0이어야 함)

   - 등분산이 아니거나 일정패턴을 보이면 독립이 아닌 것
   - 오차간의 상관관계가 없다는 `Cov(ㅌi,ㅌi+1)=0`이라는 의미, 상관관계가 존재하면 상수가 아니라 계산식이 됨
   - 계열 상관성의 종류

   ![image](https://user-images.githubusercontent.com/58683097/70901022-533fb780-203d-11ea-9a51-56300a62cc07.png)

   - 계열 상관성 확인(Durbin-Watson) 

     - 양의계열 상관 : d = 0, p=1
     - 음의계열 상관 : d = 4, p=-1
     - 계열상관이 없을 경우 : d=2, p=0

     => DW검사할 땐, d값이 2에 근사한지 확인, 귀무가설은 상관관계가 없다(p값이 0이다)

5. 다중공선성 : 독립변수간의 독립성

   - 독립적이다는 말은 인과관계와 관련된 말. 즉, 상관계수만 구하고 인과관계를 판단해선 안됨

   - 다중공선성 확인 방법

     - 기울기 계수의 낮은 통계적 유의성 + 유의한 F검정값 + 높은 결정계수

     - 독립변수간 상관계수가 높은 것 (기준은 0.7이상이면 의심)

     - 분산팽창요인 (VIF) : 다중공선성이 추정기울기 계수의 표준오차를 얼마나 증가시켰는지를 측정하는 지표

       ▷ 공식 : `VIF = 1/1-R^2`

       ▷ 보통 결정계수가 잘나와도 65% ~ 70%정도 (VIF가 3~4.5정도)이기 때문에, VIF값이 5<VIF<10이면 다중공선성 의심, VIF>10이면 다중공선성이 심각하다고 볼 수 있음

       ▷ 계수가 많을 경우 VIF값이 크게 나옴 (갯수를 줄여야 함. stepwise 등 사용해 변수선택)

6. 회귀분석에서의 설명력(결정계수, R^2)

   - 결정계수 : 회귀제곱합(SSR)/전체제곱합(SST) >> 전체모형이 차지하는 부분에서 설명가능한 부분의 비중을 알 수 있게 됨![image](https://user-images.githubusercontent.com/58683097/70901982-2a202680-203f-11ea-9df7-a232fa9fc1bb.png)

7. 변수선택방법 

   - 전진 선택법 (forward selection): 절편만 있는 모델에서 기준 통계치를 가장 많이 개선시키는 변수를 차례로 추가하는 방법

   - 후진 제거법(backward elimination): 모든 변수가 포함된 모델에서 가장 도움이 되지 않는 변수(p값)를 하나씩 제거하는 방법

   - 단계 선택법(stepwise selection): 모든 변수가 포함된 모델에서 출발하고 기준통계치에 가장 도움이 되지 않는 변수를 삭제하거나 모델에서 빠져있는 변수 중에서 기준 통계치를 가장 개선시키는 변수를 추가함. 이러한 변수의 추가 또는 제거를 반복함.

     ​																								출처 : https://rpago.tistory.com/15

8. 쿡의 거리

   - 평균에 가까울 수록 값이 적게 나옴

     ![image](https://user-images.githubusercontent.com/58683097/70902916-22fa1800-2041-11ea-827b-db7d4d4dc7d2.png)

     - 잔차가 적은 데이터 : 앞쪽  	 	잔차가 큰 데이터 : 뒤쪽
     - 뒤로 갈 수록 영향력이 큼
     - 우측 상단 부분(잔차 0.5이상)에 들어가면 이상치

     