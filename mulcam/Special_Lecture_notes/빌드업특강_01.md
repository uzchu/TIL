# Build up 특강

## 알고리즘

### Abstract

##### 예측 모델링 (Predictive Modeling)

- 감독 세분화 (Supervised Segmen)
  - Supervised : target value를 가지고 있음을 의미함
  - Segmentation : 데이터를 조각내가며 미래를 예측하겠다는 의미

##### 예측 모델링 기법

- 엔트로피
- 의사결정 나무



### Model

##### 모델 (Model)

- 어떤 목적 달성을 위해 실 세계를 **단순**하게 표현한 것
- 목적에 따라 관련없는 정보는 **생략** (feature selection)

##### 설명 모델링

- 값의 추정이 아닌 현상이나 절차를 쉽게 설명 ( 블랙박스모델이 아닌 것..)

##### 모델유도

- 데이터로부터 모델을 만드는 것 
  (**귀납법**(철학), **범용규칙**(통계학), **유도알고리즘, 학습자**(머신러닝 등) 다 같은 뜻임)



#### 감독 세분화(Supervised Segmentation)

##### 엔트로피(Entropy)

$$
entropy = -p_1log(p_1) - p_2log(p_2) - ...
$$

- p<sub>i</sub>는 집합 안에서 속성 i의 확률
- 음수를 붙이는 이유는 log에 나눗셈값이 들어가면 음수로 나와서..

- entropy값이 0이면 완전 분리됨 (1일땐 반반씩 있음)

##### 정보증가량(Information Gain)

$$
IG(parents, childeren) = entropy(parents) - [p(c_1)*entropy(c_1) +p(c_2)*entropy(c_2)+ ...]
$$

- 예시
  - IG : 대출고객 중 상환고객/비상환고객을 예측하는 모델에서 직장의 영향력
  - entropy(parents) : 상환고객/비상환고객 전부 섞여 있는 부모 노드의 엔트로피
  - c<sub>i</sub> : 속성 종류 ex) 대기업, 중소기업 등등..
  - entropy(c<sub>i</sub>) : ex)대기업다니는 고객 중 상환고객일 확률, 비상환고객일 확률 이용해 entropy구함
  - entropy(c<sub>i</sub>)에 p(c<sub>i</sub>) 을 곱해주는 이유 : 각각의 sample갯수가 다르기 때문에 값을 보정해  주려고 (ex) 대기업 20개 sample, 중소기업 50개 sample 등..



#### 의사결정트리모델에서 정보증가량(ig)은 entropy를 알면 구할 수 있고, entropy를 구하기 위해선 확률을 알아야하며, 확률을 구하기 위해선 데이터가 셀 수 있는 데이터야 한다 따라서 데이터 전처리를 할 때 셀 수 있는 데이터로 바꿔주는 작업이 필요하다!

`>`이러한 과정이 머리속에 그려져야 함



두개로 분류하기 위해 entropy개념만 쓰일까? **No!!**

의사결정모델을 더 좋은 모델로 업그레이드 시키기위해 entropy대신 다른 개념을 가져와서 넣으면 됨



**적절한 수학공식**을 가져오고, 그게 **내가 만든 모델에 왜 사용되는지** 설명하는 것이 모델개발에서의 **창의력**

그리고 이 창의력은 경험으로 95% 커버가능하기 때문에 **많은 공부가 필요함**



툴베이스 분석 공부보다 수학 및 프로그래밍을 기반의 공부가 더 중요한데, 이유는 수학 논리를 가져다 쓰면 객관성이 입증되어 내가 만든 모델의 설득력이 높아지기 때문이다. 수학공식을 가져와서 내가 만든 모델에 왜 썼는지만 설명하면 이후 반박은 공식적으로 증명된 수학모델과의 싸움이 되기 때문!

또한 프로그래밍이 중요한 이유는 이미 프로그래밍으로 만들어진 분석모델에서 원하는 부분만 바꿀 수 있도록, 코드를 읽을 수 있어야 하기 때문이다.

ex) decision tree 안에 Information gain 안에 entropy 대신 다른 수학개념을 가져다 쓸 때 바꿔치기 할 수 있어야 해서





















